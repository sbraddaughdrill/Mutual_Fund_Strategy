# TODO: Add comment
# 
# Author:  Brad
# File:    Analysis.R
# Version: 1.0
# Date:    03.17.2013
# Purpose: Analyze test
#
###############################################################################

###############################################################################
# INITIAL SETUP;
cat("SECTION: INITIAL SETUP", "\n")
###############################################################################

rm(list=ls(all=TRUE))                              #Clear workspace

Sys.setenv(R_HISTSIZE=500)                         #Limit History so that it never contains more than 50 lines

repo <- c("http://cran.us.r-project.org")
options(repos=structure(repo))
options(install.packages.check.source=FALSE)
options(StringsAsFactors=FALSE)                    #String as factors is False -- used for read.csv
#options(max.print=99999)                                   
options(max.print=500)                               #Default maxprint option

#memory.limit(size=2047)                              #Memory limit default
#memory.limit(size=3000)                              #Increase memory limit to 3000 mb (3 gb)

#Set location (HOME=1,WORK=2)
#Location <- 1
Location <- 2

if (Location==1)
{
  #HOME
  
  setwd("C:/Research_temp/")                                                                                                  #Set working directory
  input_directory <- normalizePath("C:/Users/Brad/Dropbox/Research/3rd-Year_Paper/Data/",winslash="\\", mustWork=NA)      #Create data input directory
  output_directory <- normalizePath("C:/Research_temp/",winslash="\\", mustWork=NA)                                       #Create data output directory
  function_directory <- normalizePath("C:/Users/Brad/Dropbox/Research/R/",winslash="\\", mustWork=NA)                     #Create function directory
  #package_directory <- normalizePath("C:/Users/Brad/Documents/R/win-library/2.15/",winslash="\\", mustWork=NA)            #Create package directory
  treetag_directory <- normalizePath("C:/TreeTagger",winslash="\\", mustWork=NA)                                          #Create treetag directory
  
} else if (Location==2)
{
  #WORK
  
  setwd("C:/Research_temp/")                                                                                                  #Set working directory
  input_directory <- normalizePath("C:/Users/bdaughdr/Dropbox/Research/3rd-Year_Paper/Data/",winslash="\\", mustWork=NA)  #Create data input directory
  output_directory <- normalizePath("C:/Research_temp/",winslash="\\", mustWork=NA)                                       #Create data output directory
  function_directory <- normalizePath("C:/Users/bdaughdr/Dropbox/Research/R/",winslash="\\", mustWork=NA)                 #Create function directory
  #package_directory <- normalizePath("C:/Users/bdaughdr/Documents/R/win-library/2.15/",winslash="\\", mustWork=NA)        #Create package directory
  treetag_directory <- normalizePath("C:/TreeTagger",winslash="\\", mustWork=NA)                                          #Create treetag directory
  
} else
{
  cat("ERROR ASSIGNING DIRECTORIES", "\n")
  
}
rm(Location)


###############################################################################
# FUNCTIONS;
cat("SECTION: FUNCTIONS", "\n")
###############################################################################

source(file=paste(function_directory,"functions_db.R",sep=""),echo=FALSE)
source(file=paste(function_directory,"functions_statistics.R",sep=""),echo=FALSE)
source(file=paste(function_directory,"functions_text_analysis.R",sep=""),echo=FALSE)
source(file=paste(function_directory,"functions_utilities.R",sep=""),echo=FALSE)


###############################################################################
# LIBRARIES;
cat("SECTION: LIBRARIES", "\n")
###############################################################################

update.packages(ask=FALSE, checkBuilt=TRUE)

#Load External Packages
#external_packages <- c("arules","bigalgebra","biganalytics","bigmemory","bigtabulate","clv","ff","foreach",
#                       "pracma","snow","snowfall","synchronicity","XML")
external_packages <- c("compare","cwhmisc","data.table","fastmatch","foreign","formatR","gdata","gtools",
                       "Hmisc","koRpus","mitools","pbapply","plyr","R.oo","rJava","RWeka","RWekajars",
                       "Snowball","sqldf","stringr","tcltk","tm")
invisible(unlist(sapply(external_packages,load_external_packages, repo_str=repo, simplify=FALSE, USE.NAMES=FALSE)))
installed_packages <- list_installed_packages(external_packages)

###############################################################################
# PREALLOCATE DATA;
cat("SECTION: PREALLOCATE DATA", "\n")
###############################################################################

#Create base column table
temp_data_cols <- as.data.frame(matrix(NA, ncol=7, nrow=200),stringsAsFactors=FALSE)
colnames(temp_data_cols) <- c("order","isnum","ischar","isdate","isfactor","colnames","desc")
temp_data_cols[,1] <- as.numeric(temp_data_cols[,1])
temp_data_cols[,2] <- as.numeric(temp_data_cols[,2])
temp_data_cols[,3] <- as.numeric(temp_data_cols[,3])
temp_data_cols[,4] <- as.numeric(temp_data_cols[,4])
temp_data_cols[,5] <- as.numeric(temp_data_cols[,5])
temp_data_cols[,6] <- as.character(temp_data_cols[,6])
temp_data_cols[,7] <- as.character(temp_data_cols[,7])

#Files table
#files[,1] <- c("1999.csv","2000.csv","2001.csv","2002.csv","2003.csv","2004.csv","2005.csv","2006.csv","2007.csv","2008.csv")
file_list <- c("Target_aggregate.csv")
files_cols_count <- 2
files_cols <- temp_data_cols[1:files_cols_count,]
files_cols[1,] <- data.frame(order=1,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="filename",stringsAsFactors=FALSE)
files_cols[2,] <- data.frame(order=2,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="filepath",stringsAsFactors=FALSE)
files <- as.data.frame(matrix(NA, ncol=files_cols_count, nrow=length(file_list)),stringsAsFactors=FALSE)
colnames(files) <- files_cols[,6]
files <- format_function(files,files_cols)


#Populate Percentiles table

#Note: If Confidence_Level is 0.900, 
#          word_grand  -  this means that if the word is the 10% most common occuring or higher overall word, it is removed (90% removed/10% kept)
#          word_unique -  this means that if the word is the 10% most common occuring or higher unique word, it is removed (90% removed/10% kept)
#          id_unique -  this means that if a word is in 10% of the ids or higher, it is removed  (90% removed/10% kept)
#Note: If Confidence_Level is 0.050, 
#          word_grand  -  this means that if the word is the 95% most common occuring or higher overall word, it is removed (5% removed/95% kept)
#          word_unique -  this means that if the word is the 95% most common occuring or higher unique word, it is removed (5% removed/95% kept)
#          id_unique -  this means that if a word is in 95% of the ids or higher, it is removed  (5% removed/95% kept)
# As confidence level increases, the words in the dictionary should decrease.
# A smaller dicitonary means that that smilarity will be less because there are fewer possible words that could be common between two texts 
# Thus, as confidence level increases, the smilarity scores will decrease

#Percentiles table
percentiles_cols_count <- 8
percentiles_cols <- temp_data_cols[1:percentiles_cols_count,]
percentiles_cols[1,] <- data.frame(order=1,isnum=1,ischar=0,isdate=0,isfactor=0,colnames="Confidence_Level",stringsAsFactors=FALSE)
percentiles_cols[2,] <- data.frame(order=2,isnum=1,ischar=0,isdate=0,isfactor=0,colnames="Confidence_Pct",stringsAsFactors=FALSE)
percentiles_cols[3,] <- data.frame(order=3,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="Confidence_lbl",stringsAsFactors=FALSE)
percentiles_cols[4,] <- data.frame(order=4,isnum=1,ischar=0,isdate=0,isfactor=0,colnames="Significance_Level",stringsAsFactors=FALSE)
percentiles_cols[5,] <- data.frame(order=5,isnum=1,ischar=0,isdate=0,isfactor=0,colnames="Significance_Pct",stringsAsFactors=FALSE)
percentiles_cols[6,] <- data.frame(order=6,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="Significance_lbl",stringsAsFactors=FALSE)
percentiles_cols[7,] <- data.frame(order=7,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="Column_lbl",stringsAsFactors=FALSE)
percentiles_cols[8,] <- data.frame(order=8,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="Column_DV",stringsAsFactors=FALSE)

#percentile_vals <- c(0.990,0.950,0.900)
percentile_vals <- c(0.900,0.750,0.500,0.250,0.100,0.050)

percentiles <- as.data.frame(matrix(NA, ncol=percentiles_cols_count, nrow=length(percentile_vals)),stringsAsFactors=FALSE)
colnames(percentiles) <- percentiles_cols[,6]

percentiles[,"Confidence_Level"] <- format(as.double(percentile_vals), digits=3)
percentiles[,"Confidence_Pct"] <- as.double(percentiles[,"Confidence_Level"])*100
percentiles[,"Confidence_lbl"] <- formatC(percentiles[,"Confidence_Pct"],format="f", digits=1,width=4,  flag="0")
percentiles[,"Significance_Level"] <- 1-as.double(percentiles[,"Confidence_Level"])
percentiles[,"Significance_Pct"] <- as.double(percentiles[,"Significance_Level"])*100
percentiles[,"Significance_lbl"] <- formatC(percentiles[,"Significance_Pct"], format="f",digits=1, width=5,  flag="0")
percentiles[,"Confidence_lbl"] <-  paste(gsub(pattern="\\.", replacement="", x=percentiles[,"Confidence_lbl"]),"pct",sep="")
percentiles[,"Significance_lbl"] <- paste(gsub(pattern="\\.", replacement="", x=percentiles[,"Significance_lbl"]),"pct",sep="")
percentiles[,"Column_lbl"] <- paste("Word_Cutoff_",percentiles[,"Confidence_lbl"],sep="")
percentiles[,"Column_DV"] <- paste("Word_DV_",percentiles[,"Confidence_lbl"],sep="")
percentiles <- format_function(percentiles,percentiles_cols)
percentiles <- percentiles[order(percentiles[,"Confidence_Level"]),]


#Readability columns table
readbl_vars_cols_count <- 4
readbl_vars_cols <- temp_data_cols[1:readbl_vars_cols_count,]
readbl_vars_cols[1,] <- data.frame(order=1,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="column",stringsAsFactors=FALSE)
readbl_vars_cols[2,] <- data.frame(order=2,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="suffix",stringsAsFactors=FALSE)
readbl_vars_cols[3,] <- data.frame(order=3,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="readabilitystats_table",stringsAsFactors=FALSE)
readbl_vars_cols[4,] <- data.frame(order=4,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="token_table",stringsAsFactors=FALSE)
readbl_vars <- as.data.frame(matrix(NA, ncol=readbl_vars_cols_count, nrow=2),stringsAsFactors=FALSE)
colnames(readbl_vars) <- readbl_vars_cols[,6]

#Readability statistics table
readbl_all_df_cols_count <- 5
readbl_all_df_cols <- temp_data_cols[1:readbl_all_df_cols_count,]
readbl_all_df_cols[1,] <- data.frame(order=1,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="index",stringsAsFactors=FALSE)
readbl_all_df_cols[2,] <- data.frame(order=2,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="flavour",stringsAsFactors=FALSE)
readbl_all_df_cols[3,] <- data.frame(order=3,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="raw",stringsAsFactors=FALSE)
readbl_all_df_cols[4,] <- data.frame(order=4,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="grade",stringsAsFactors=FALSE)
readbl_all_df_cols[5,] <- data.frame(order=5,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="age",stringsAsFactors=FALSE)
readbl_all_df <- as.data.frame(matrix(NA, ncol=readbl_all_df_cols_count, nrow=44),stringsAsFactors=FALSE)
colnames(readbl_all_df) <- readbl_all_df_cols[,6]
readbl_all_df <- format_function(readbl_all_df,readbl_all_df_cols)

#Sample data input columns
#sample_data_input_cols_count <- 8
#sample_data_input_cols <- temp_data_cols[1:sample_data_input_cols_count,]
#sample_data_input_cols[1,] <- data.frame(order=1,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="ID",desc="input",stringsAsFactors=FALSE)
#sample_data_input_cols[2,] <- data.frame(order=2,isnum=1,ischar=0,isdate=0,isfactor=0,colnames="yr",desc="input",stringsAsFactors=FALSE)
#sample_data_input_cols[3,] <- data.frame(order=3,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="crsp_fundno",desc="input",stringsAsFactors=FALSE)
#sample_data_input_cols[4,] <- data.frame(order=4,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="investment_objective_f",desc="input",stringsAsFactors=FALSE)
#sample_data_input_cols[5,] <- data.frame(order=5,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="investment_strategy_f",desc="input",stringsAsFactors=FALSE)
#sample_data_input_cols[6,] <- data.frame(order=6,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="investment_objective_strategy_f",desc="input",stringsAsFactors=FALSE)
#sample_data_input_cols[7,] <- data.frame(order=7,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="principal_risks_f",desc="input",stringsAsFactors=FALSE)
#sample_data_input_cols[8,] <- data.frame(order=8,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="File",desc="input",stringsAsFactors=FALSE)

#Tokens table
tokens_all_cols_count <- 5
tokens_all_cols <- temp_data_cols[1:tokens_all_cols_count,]
tokens_all_cols[1,] <- data.frame(order=1,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="ID",stringsAsFactors=FALSE)
tokens_all_cols[2,] <- data.frame(order=2,isnum=1,ischar=0,isdate=0,isfactor=0,colnames="yr",stringsAsFactors=FALSE)
tokens_all_cols[3,] <- data.frame(order=3,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="token",stringsAsFactors=FALSE)
tokens_all_cols[4,] <- data.frame(order=4,isnum=0,ischar=1,isdate=0,isfactor=0,colnames="desc",stringsAsFactors=FALSE)
tokens_all_cols[5,] <- data.frame(order=5,isnum=1,ischar=0,isdate=0,isfactor=0,colnames="Remove",stringsAsFactors=FALSE)

#==============================================================================;
cat("SECTION: POPULATE DATA", "\n")
#==============================================================================;

#List variables to compute readability statistics and the suffixes used for each in the sample_data_statistics_cols table
readbl_vars[,1] <- c("investment_objective_strategy_f","principal_risks_f")
readbl_vars[,2] <- c("_iois","_pr")
readbl_vars[,3] <- c("read_stats_ios_f","read_stats_pr_f")
readbl_vars[,4] <- c("tokens_all_ios_f","tokens_all_pr_f")

measures <- c("word_grand","word_unique","id_unique")

###############################################################################
cat("SECTION: IMPORT DATA", "\n")
###############################################################################

files[,1] <-  file_list
files[,2] <-  unlist(mapply(merge_cols_function,col_one=input_directory,col_two=files[,1],separator="", SIMPLIFY=FALSE,USE.NAMES=FALSE))

#sample_data_all <- as.data.frame(matrix(NA, ncol=sample_data_input_cols_count, nrow=300000),stringsAsFactors=FALSE)
#colnames(sample_data_all) <- sample_data_input_cols[,6]
#sample_data_all <- format_function(sample_data_all,sample_data_input_cols)

# for (j in 1:nrow(files))
# {
#   #j <- 1
#   #j <- 2
#   
#   sample_data_NA_row_index <- which(rowSums(is.na(sample_data_all[,1:ncol(sample_data_all)]))==ncol(sample_data_all))
#   sample_data_NA_row_first <- as.numeric(min(sample_data_NA_row_index))
#   
#   csv_j_row_count <- as.numeric(nrow(read.csv(file=files[j,2],header=TRUE,na.strings="NA",stringsAsFactors=FALSE)))
#   csv_j_headers <- names(read.csv(file=files[j,2],header=TRUE,na.strings="NA",stringsAsFactors=FALSE))
#   
#   #sample_data_cols2 <- sample_data_cols[1:2]
#   #zzz1 <- sapply(sample_data_cols,import_across_row_function, 
#   #               data_temp=sample_data_all, file_temp=files[j,2], row_NA_first_temp=sample_data_NA_row_first,temp_row_count=csv_j_row_count,temp_headers=csv_j_headers,
#   #               simplify=FALSE, USE.NAMES=FALSE)
#   #zzz2 <- ldply(zzz1, data.frame)
#   #colnames(zzz1) <- sample_data_cols
#   
#   if (sample_data_NA_row_first==1)
#   {
#     sample_data_file_col_num <- as.numeric(match("File",names(sample_data_all)))
#     sample_data_all[1:csv_j_row_count,sample_data_file_col_num] <- files[j,1]
#     
#     for (k in 1:ncol(sample_data_all))
#       #for (k in 1:nrow(sample_data_input_cols))
#     {
#       #k <- 1
#       csv_k_col_num  <- as.numeric(match(sample_data_input_cols[k,6],csv_j_headers))
#       
#       if(!(is.na(csv_k_col_num)))
#       {
#         #cat(sample_data_input_cols[k,6]," is in ",files[j,2], "\n")
#         
#         sample_data_all[,k] <- append(as.character(read.csv(file=files[j,2],header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[1:csv_j_row_count,csv_k_col_num]),
#                                       as.character(sample_data_all[(csv_j_row_count+1):nrow(sample_data_all),k]))
#         
#       } else
#       {
#         #cat(sample_data_input_cols[k,6]," not in ",files[j,2], "\n")
#       }
#     }
#     
#   } else if (sample_data_NA_row_first>1)
#   {
#     sample_data_file_col_num <- as.numeric(match("File",names(sample_data_all)))
#     sample_data_all[sample_data_NA_row_first:(sample_data_NA_row_first+csv_j_row_count-1),sample_data_file_col_num] <- files[j,1]
#     
#     for (k in 1:ncol(sample_data_all))
#       #for (k in 1:nrow(sample_data_input_cols))
#     {
#       #k <- 1
#       #k <- 70
#       csv_k_col_num  <- as.numeric(match(sample_data_input_cols[k,6],csv_j_headers))
#       
#       if(!(is.na(csv_k_col_num)))
#       {
#         #cat(sample_data_input_cols[k,6]," is in ",files[j,2], "\n")
#         
#         sample_data_all[,k] <- append(append(as.character(sample_data_all[1:(sample_data_NA_row_first-1),k]),
#                                              as.character(read.csv(file=files[j,2],header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[1:(csv_j_row_count),csv_k_col_num])),
#                                       as.character(sample_data_all[(sample_data_NA_row_first+csv_j_row_count):nrow(sample_data_all),k]))
#         
#       } else
#       {
#         #cat(sample_data_input_cols[k,6]," not in ",files[j,2], "\n")
#       }
#     }
#     
#   } else
#   {
#     cat("ERROR!!", "\n")
#     
#   }
#   
#   #Format data
#   sample_data_all <- format_function(sample_data_all,sample_data_input_cols)
# 
#   ###############################################################################
#   # OUTPUT DATA;
#   ###############################################################################
#   
#   #Create temp data.frame of Sample data where not NA after each iteration
#   temp_output_name1 <- paste("sample_data", formatC(j, width=6, format="d", flag="0"), sep="")
#   #assign(temp_output_name1, sample_data_all[!(rowSums(is.na(sample_data_all[,1:ncol(sample_data_all)]))==ncol(sample_data_all)),], envir=.GlobalEnv)
#   write.csv(sample_data_all[!(rowSums(is.na(sample_data_all[,1:ncol(sample_data_all)]))==ncol(sample_data_all)),], file=paste(output_directory,temp_output_name1,".csv",sep=""))
#   
#   
#   #Create temp data.frame of for each CSV file
#   temp_output_name2 <- paste("input_csv", formatC(j, width=6, format="d", flag="0"), sep="")
#   #assign(temp_output_name2, sample_data_all[sample_data_all[,sample_data_file_col_num]==files[j,1] & !is.na(sample_data_all[,sample_data_file_col_num]),], envir=.GlobalEnv)
#   write.csv(sample_data_all[sample_data_all[,sample_data_file_col_num]==files[j,1] & !is.na(sample_data_all[,sample_data_file_col_num]),], file=paste(output_directory,temp_output_name2,".csv",sep=""))
#   
#   ###############################################################################
#   # CREATE PROGRESS OUTPUTS;
#   ###############################################################################
#   
#
#   progress_function(outer_loop_count=j, outer_loop_start_val=1, outer_loop_end_val=nrow(files), inner_loop_count=1, inner_loop_start_val=1, inner_loop_end_val=1)
#   
# }
# 
# rm2(sample_data_NA_row_index,temp_output_name1,temp_output_name2,csv_k_col_num)

sample_data <- read.csv(file=files[1,2],header=TRUE,na.strings="NA",stringsAsFactors=FALSE)
for(i in which(sapply(sample_data,class)=="character"))
{
  sample_data[[i]] = trim(sample_data[[i]])
}
for (i in 1:ncol(sample_data))
{
  sample_data[,i] <- unknownToNA(sample_data[,i], unknown=c("",".","NA_character_","NA_Real_","NA",NA),force=TRUE)
  sample_data[,i] <- ifelse(is.na(sample_data[,i]),NA, sample_data[,i])
} 


sample_data <- sample_data[rowSums(is.na(sample_data[,1:ncol(sample_data)]))<ncol(sample_data),]
sample_data <- sample_data[!(is.na(sample_data[,"investment_objective_f"])) | 
                             !(is.na(sample_data[,"investment_strategy_f"])) | 
                             !(is.na(sample_data[,"principal_risks_f"])),]

row.names(sample_data)  <- seq(nrow(sample_data))

#sample_data_all <- format_function(sample_data_all,sample_data_input_cols)

#Merge investment_objective_f and investment_strategy_f and place in investment_objective_strategy_f
merge_io_is <- unlist(mapply(merge_cols_function,
                             col_one=sample_data[,"investment_objective_f"],
                             col_two=sample_data[,"investment_strategy_f"],
                             separator="\n\n", SIMPLIFY=FALSE,USE.NAMES=FALSE))

sample_data_all <- data.frame(sample_data[,c("wficn","yr")],
                              investment_objective_strategy_f=merge_io_is,
                              principal_risks_f=sample_data[,"principal_risks_f"],stringsAsFactors=FALSE)

rm2(merge_io_is,sample_data)

#Trim strings
sample_data_all[,"investment_objective_strategy_f"] <- trim(sample_data_all[,"investment_objective_strategy_f"])
sample_data_all[,"principal_risks_f"] <- trim(sample_data_all[,"principal_risks_f"])

#Remove multiple spaces (run a couple times)
for (a in 1:5)
{
  #a <- 1
  sample_data_all[,"investment_objective_strategy_f"] <- gsub(pattern=" {2,}", replacement=" ", x=sample_data_all[,"investment_objective_strategy_f"])
  sample_data_all[,"principal_risks_f"]  <- gsub(pattern=" {2,}", replacement=" ", x=sample_data_all[,"principal_risks_f"])
  
}

#Remove double hyphens
for (a in 1:5)
{
  #a <- 1
  sample_data_all[,"investment_objective_strategy_f"] <- gsub(pattern="--", replacement="-", x=sample_data_all[,"investment_objective_strategy_f"])
  sample_data_all[,"principal_risks_f"]  <- gsub(pattern="--", replacement="-", x=sample_data_all[,"principal_risks_f"])
  
}

#Remove sections that have less than 100 words
sample_data_all_ios_len  <- pbsapply(sample_data_all[,"investment_objective_strategy_f"],
                                     function(x) {return(length(strsplit(x,' ')[[1]]))}, 
                                     simplify=FALSE, USE.NAMES=FALSE)

sample_data_all_pr_len  <- pbsapply(sample_data_all[,"principal_risks_f"],
                                    function(x) {return(length(strsplit(x,' ')[[1]]))}, 
                                    simplify=FALSE, USE.NAMES=FALSE)

sample_data_all_trim  <- data.frame(sample_data_all,
                                    ios_word_count=unlist(sample_data_all_ios_len),
                                    pr_word_count=unlist(sample_data_all_pr_len),stringsAsFactors=FALSE)

sample_data_all_trim2 <- sample_data_all_trim[(sample_data_all_trim[,"ios_word_count"]>=100 & 
                                                 sample_data_all_trim[,"pr_word_count"]>=100) ,]
sample_data_all_trim3 <- subset(sample_data_all_trim2,select=-c(ios_word_count,pr_word_count))

write.csv(sample_data_all_trim3,file=paste(output_directory,"sample_data_all.csv",sep=""),na="",quote=TRUE,row.names=FALSE)

rm2(sample_data_all,sample_data_all_ios_len,sample_data_all_pr_len)
rm2(sample_data_all_trim,sample_data_all_trim2,sample_data_all_trim3)

###############################################################################
cat("SECTION: COMPUTE READABILITY STATISTICS", "\n")
###############################################################################

sample_data_all <- read.csv(file=paste(output_directory,"sample_data_all.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)
for(i in which(sapply(sample_data_all,class)=="character"))
{
  sample_data_all[[i]] = trim(sample_data_all[[i]])
}
for (i in 1:ncol(sample_data_all))
{
  sample_data_all[,i] <- unknownToNA(sample_data_all[,i], unknown=c("",".","NA_character_","NA_Real_","NA",NA),force=TRUE)
  sample_data_all[,i] <- ifelse(is.na(sample_data_all[,i]),NA, sample_data_all[,i])
} 

sample_data_all <- data.frame(ID=seq(1,nrow(sample_data_all)),sample_data_all,stringsAsFactors=FALSE)
sample_data_all[,"ID"] <- paste("", formatC(sample_data_all[,"ID"] , width=6, format="d", flag="0"), sep="")


Dale.Chall_word_list <- read.csv(file=paste(input_directory,"DaleChall_word_list.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)
for(i in which(sapply(Dale.Chall_word_list,class)=="character"))
{
  Dale.Chall_word_list[[i]] = trim(Dale.Chall_word_list[[i]])
}
for (i in 1:ncol(Dale.Chall_word_list))
{
  Dale.Chall_word_list[,i] <- unknownToNA(Dale.Chall_word_list[,i], unknown=c("",".","NA_character_","NA_Real_","NA",NA),force=TRUE)
  Dale.Chall_word_list[,i] <- ifelse(is.na(Dale.Chall_word_list[,i]),NA, Dale.Chall_word_list[,i])
} 

keep_one_letter_words <- c("I")
keep_one_letter_ratings <- c("A","B","C","D","P")
keep_one_letter_tokens <- sort(c(keep_one_letter_words,keep_one_letter_ratings))

keep_two_letter_words <- c("AM","AN","AS","AT","BE","BY","DO","EG","EX","HA","ID","IE","IF","IN","IS",
                           "IT","MY","NO","OF","ON","OR","QA","RD","SO","SP","TM","TO","TV","UM","UN",
                           "UP","US","VP","WE")
keep_state_abbreviations <- c("AK","AL","AR","AZ","CA","CF","CL","CO","CT","DC","DE","DL","FL","GA","HA",
                              "HI","IA","ID","IL","IN","KA","KS","KY","LA","MA","MC","MD","ME","MI","MN",
                              "MO","MS","MT","NB","NC","ND","NE","NH","NJ","NM","NV","NY","OH","OK","OR",
                              "PA","RI","SC","SD","TN","TX","UT","VA","VT","WA","WI","WN","WS","WV","WY")
keep_two_letter_ratings <- c("AA","BA","BB","CA","CC")
keep_two_letter_tokens <- sort(c(keep_two_letter_words,keep_state_abbreviations,keep_two_letter_ratings))

# remove stopwords
myStopwords <- c(stopwords('english'),stopwords('SMART'),"available", "via")
myStopwords_no_punct <- gsub(pattern="[^[[:alnum:][:space:]]", replacement="", x=myStopwords)
myStopwords_all <- c(myStopwords,myStopwords_no_punct)
myStopwords_all <- sort(myStopwords_all)
myStopwords_all <- unique(myStopwords_all, incomparables=FALSE)
myStopwords_all <- toupper(myStopwords_all)

#idx <- which(myStopwords_all %in% c("R",keep_one_letter_tokens,keep_two_letter_tokens))
idx <- which(myStopwords_all %in% c("R"))
myStopwords_all <- myStopwords_all[-idx]

tagged_text_desc_stats <- c("lines","chars.no.space","letters.only","digits","normalized.space")
hyph_text_en_desc_stats <- c("num.syll")
readability_stats <- c("ARI", "Coleman.Liau","Flesch.Kincaid", "FOG","SMOG")
readability_desc_stats <- c("sentences","words","all.chars","punct","conjunctions","prepositions","pronouns",
                            "foreign","FOG.hard.words","TTR","sntc.per.word","avg.sentc.length","avg.word.length",
                            "avg.syll.word","sntc.per100","syll.per100","lett.per100")
readability_all_stats <- c(tagged_text_desc_stats,hyph_text_en_desc_stats,readability_stats,readability_desc_stats)

#token_stats <- c("token","desc","stop","stem")
token_stats <- c("token","desc")

#sample_data_all <- sample_data_all[1:50,]


for (l in 1:nrow(readbl_vars))
{
  #l <- 1
  #l <- 2
  
  readability_all_stats_temp <- paste(readability_all_stats, readbl_vars[l,2], sep="")
  readability_all_stats_temp <- gsub(pattern="\\.", replacement="_", x=readability_all_stats_temp)
  
  sample_results <- pbsapply(sample_data_all[,readbl_vars[l,1]],compute_readability_stats,
                             tagged_text_desc_measures=tagged_text_desc_stats,
                             hyph_text_en_desc_measures=hyph_text_en_desc_stats,
                             readability_measures=readability_stats,
                             readability_desc_measures=readability_desc_stats,
                             token_measures=token_stats,
                             dc_word_list=Dale.Chall_word_list,
                             simplify=FALSE, USE.NAMES=FALSE)
  
  sample_read_stats <- pblapply(sample_results, "[[","readstats")
  sample_read_stats <- pblapply(seq_along(sample_read_stats), function(x) data.frame(ID=x,sample_read_stats[x],stringsAsFactors=FALSE))
  sample_read_stats_df <- as.data.frame(do.call(rbind, sample_read_stats),stringsAsFactors=FALSE)
  
  rm2(sample_read_stats)

  sample_read_stats_df[,1] <- paste("", formatC(sample_read_stats_df[,1], width=6, format="d", flag="0"), sep="")
  colnames(sample_read_stats_df) <- c("ID",readability_all_stats_temp)
  
  #sample_data_all_temp <- cbind(subset(sample_data_all,select=c("ID","wficn","yr")),subset(sample_data_all,select=c(readbl_vars[l,1])))
  #sample_data_all_temp <- subset(sample_data_all,select=c("ID","yr","crsp_fundno"))
  
  sample_read_stats_df_merge <- merge(sample_data_all[,c("ID","wficn","yr")], sample_read_stats_df, by.x=c("ID") , by.y=c("ID"), all.x=TRUE, all.y=FALSE, sort=TRUE, suffixes=c(".x",".y"),incomparables=NA)
  
  rm2(sample_read_stats_df)
 
  sample_read_stats_df_merge <- subset(sample_read_stats_df_merge,select=-c(ID))
  
  write.csv(sample_read_stats_df_merge, file=paste(output_directory,readbl_vars[l,3],".csv",sep=""),row.names=FALSE)
  
  rm2(sample_read_stats_df_merge)
 
  sample_tokens <- pblapply(sample_results, "[[","tokens") 
  sample_tokens <- pblapply(seq_along(sample_tokens), function(x) data.frame(ID=x,sample_tokens[x],stringsAsFactors=FALSE))
  sample_tokens_df <- as.data.frame(do.call(rbind, sample_tokens),stringsAsFactors=FALSE)
  
  rm2(sample_tokens)

  sample_tokens_df[,1] <- paste("", formatC(sample_tokens_df[,1], width=6, format="d", flag="0"), sep="")
  colnames(sample_tokens_df) <- c("ID",token_stats)
  #colnames(sample_tokens_df) <- c(token_stats)
  
  sample_tokens_df_merge <- merge(sample_data_all[,c("ID","wficn","yr")], sample_tokens_df, by.x=c("ID") , by.y=c("ID"), all.x=TRUE, all.y=FALSE, sort=TRUE, suffixes=c(".x",".y"),incomparables=NA)
  
  rm2(sample_tokens_df)
 
  sample_tokens_df_merge <- subset(sample_tokens_df_merge,select=-c(ID))
  
  write.csv(sample_tokens_df_merge, file=paste(output_directory,readbl_vars[l,4],".csv",sep=""),row.names=FALSE)
  
  rm2(sample_tokens_df_merge,sample_results)

}

rm2(sample_data_all)
rm2(tagged_text_desc_stats,hyph_text_en_desc_stats,readability_stats,readability_desc_stats,readability_all_stats,token_stats)

###############################################################################
# COMPUTE SIMILARITY STATISTICS;
cat("SECTION: COMPUTE READABILITY STATISTICS", "\n")
###############################################################################

#sample_data_all <- read.csv(file=paste(output_directory,"sample_data_all.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[-c(1)]
#sample_data_all[,"ID"] <- paste("", formatC(sample_data_all[,"ID"], width=6, format="d", flag="0"), sep="")
#sample_data_all[,"crsp_fundno"] <- paste("", formatC(sample_data_all[,"crsp_fundno"], width=6, format="d", flag="0"), sep="")
#sample_data_all[,"ID"] <- as.character(sample_data_all[,"ID"])
#sample_data_all[,"yr"] <- as.integer(sample_data_all[,"yr"])
#sample_data_all[,"crsp_fundno"] <- as.character(sample_data_all[,"crsp_fundno"])
#sample_data_all[,"investment_objective_strategy_f"] <- as.character(sample_data_all[,"investment_objective_strategy_f"])
#sample_data_all[,"principal_risks_f"] <- as.character(sample_data_all[,"principal_risks_f"])
#sample_data_all[,"File"] <- as.character(sample_data_all[,"File"])

sample_data_all <- read.csv(file=paste(output_directory,"sample_data_all.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)
for(i in which(sapply(sample_data_all,class)=="character"))
{
  sample_data_all[[i]] = trim(sample_data_all[[i]])
}
for (i in 1:ncol(sample_data_all))
{
  sample_data_all[,i] <- unknownToNA(sample_data_all[,i], unknown=c("",".","NA_character_","NA_Real_","NA",NA),force=TRUE)
  sample_data_all[,i] <- ifelse(is.na(sample_data_all[,i]),NA, sample_data_all[,i])
} 

for (m in 1:nrow(readbl_vars))
{
  
  #m <- 1
  #m <- 2
  
  cat("Token table: ",readbl_vars[m,4], "\n")
  if (m==1)
  {
    
    #input_row_count <- nrow(read.csv(file=paste(output_directory,"tokens_all_ios_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE))
    #tokens_all_temp <- as.data.frame(matrix(NA, ncol=tokens_all_cols_count, nrow=input_row_count))
    #colnames(tokens_all_temp) <- tokens_all_cols[,6]
    
    #tokens_all_temp[,"ID"] <- read.csv(file=paste(output_directory,"tokens_all_ios_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[c("ID")]
    #tokens_all_temp[,"token"] <- read.csv(file=paste(output_directory,"tokens_all_ios_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[c("token")]
    #tokens_all_temp[,"desc"] <- read.csv(file=paste(output_directory,"tokens_all_ios_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[c("desc")]
    
    tokens_all_temp <- read.csv(file=paste(output_directory,"tokens_all_ios_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)
    
  } else if (m==2)
  {
    
    #input_row_count <- nrow(read.csv(file=paste(output_directory,"tokens_all_pr_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE))
    #tokens_all_temp <- as.data.frame(matrix(NA, ncol=tokens_all_cols_count, nrow=input_row_count))
    #colnames(tokens_all_temp) <- tokens_all_cols[,6]
    
    #tokens_all_temp[,"ID"] <- read.csv(file=paste(output_directory,"tokens_all_pr_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[c("ID")]
    #tokens_all_temp[,"token"] <- read.csv(file=paste(output_directory,"tokens_all_pr_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[c("token")]
    #tokens_all_temp[,"desc"] <- read.csv(file=paste(output_directory,"tokens_all_pr_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)[c("desc")]
    
    tokens_all_temp <- read.csv(file=paste(output_directory,"tokens_all_pr_f.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)
    
  } else
  {
    cat("ERROR WHEN FINDING CORRECT READABILITY COLUMN", "\n")
    
  }
  
  for(i in which(sapply(tokens_all_temp,class)=="character"))
  {
    tokens_all_temp[[i]] = trim(tokens_all_temp[[i]])
  }
  for (i in 1:ncol(tokens_all_temp))
  {
    tokens_all_temp[,i] <- unknownToNA(tokens_all_temp[,i], unknown=c("",".","NA_character_","NA_Real_","NA",NA),force=TRUE)
    tokens_all_temp[,i] <- ifelse(is.na(tokens_all_temp[,i]),NA, tokens_all_temp[,i])
  } 
  
  tokens_all_temp <- tokens_all_temp[!(rowSums(is.na(tokens_all_temp[,1:ncol(tokens_all_temp)]))==ncol(tokens_all_temp)),]
  #tokens_all_temp[,"ID"] <- paste("", formatC(tokens_all_temp[,"ID"], width=6, format="d", flag="0"), sep="")
  #tokens_all_temp <- format_function(tokens_all_temp,tokens_all_cols)
  
  #Merge in year and crsp_fundno
  #query_merge_token_yr <- ""
  #query_merge_token_yr <- paste(query_merge_token_yr, "select       a.ID, b.yr, b.crsp_fundno, a.token, a.desc, a.Remove ", sep=" ")
  #query_merge_token_yr <- paste(query_merge_token_yr, "from         tokens_all_temp a                     ", sep=" ")
  #query_merge_token_yr <- paste(query_merge_token_yr, "left join    sample_data_all b                     ", sep=" ")
  #query_merge_token_yr <- paste(query_merge_token_yr, "on           a.ID=b.ID                             ", sep=" ")
  #query_merge_token_yr <- trim(gsub(" {2,}", " ", query_merge_token_yr))
  #tokens_all_temp <- sqldf(query_merge_token_yr)
  
  #token_col_num <- as.numeric(match("token",names(tokens_all_temp)))
  
  tokens_all_temp <- tokens_all_temp[order(tokens_all_temp[,"wficn"],tokens_all_temp[,"yr"]),] 
  
  #Trim strings
  tokens_all_temp[,"token"] <- trim(tokens_all_temp[,"token"])
  
  #Upcase strings
  tokens_all_temp[,"token"] <- toupper(tokens_all_temp[,"token"])
  
  #Remove everything except letters, space, apostrophe, hyphen, and ampersand
  tokens_all_temp[,"token"] <- as.data.frame(gsub(pattern="[^[[:alnum:][:space:]'&-]", replacement=" ", x=tokens_all_temp[,"token"]))

  #Remove multiple spaces (run a couple times)
  for (a in 1:5)
  {
    #a <- 1
    tokens_all_temp[,"token"] <- gsub(pattern=" {2,}", replacement=" ", x=tokens_all_temp[,"token"])
  }
  
  #Remove numbers
  tokens_all_temp[,"token"]  <- gsub(pattern="\\d", replacement="", x=tokens_all_temp[,"token"])
  
  saved_symbols <- c("&","-","'")
  
  #Remove double apostrophe, hyphen, and ampersand
  for (a in 1:5)
  {
    #a <- 1
    tokens_all_temp[,"token"]  <- remove_duplicate_symbols(tokens_all_temp[,"token"], saved_symbols)
    
  }
  
  #Remove single, leading, and trailing symbols
  tokens_all_temp[,"token"]  <-  remove_single_symbols(tokens_all_temp[,"token"], saved_symbols)
  tokens_all_temp[,"token"]  <-  remove_leading_symbols(tokens_all_temp[,"token"], saved_symbols)
  tokens_all_temp[,"token"]  <-  remove_trailing_symbols(tokens_all_temp[,"token"], saved_symbols)
  
  #Remove single spaces (run a couple times)
  for (a in 1:5)
  {
    #a <- 1
    tokens_all_temp[,"token"]  <- gsub(pattern=" ", replacement="", x=tokens_all_temp[,"token"])
  }
  
  
  #Default Remove to NA
  tokens_all_temp[,"Remove"] <- as.numeric(rep(NA, nrow(tokens_all_temp)))
  
  #Find which rows to remove
  remove_descriptions <- c("Cardinal number","Comma","Sentence ending punctuation","Symbol",
                           "Opening bracket","Closing bracket","Quote","End quote")
  remove_punct <- c("%","&","\t","-","--","---","'",""," ")
  remove_phone <- c("1-800-XXX-XXXX","XXX-XXX-XXXX","XXX-XXXX","-XXX-XXX-XXXX")
  remove_tokens <- c(remove_punct,remove_phone)

  #keep_three_letter_ratings <- c("AAA","BAA","BBB","CAA","CCC","DDD")
  
  tokens_all_temp[,"Remove"] <- ifelse(!(tokens_all_temp$desc %in% remove_descriptions) & !(tokens_all_temp$token %in% remove_tokens), 0, 1)
  tokens_all_temp[,"Remove"] <- ifelse(((nchar(tokens_all_temp$token)==1) & !(tokens_all_temp$token %in% keep_one_letter_tokens)), 1, tokens_all_temp$Remove)
  tokens_all_temp[,"Remove"] <- ifelse(((nchar(tokens_all_temp$token)==2) & !(tokens_all_temp$token %in% keep_two_letter_tokens)), 1, tokens_all_temp$Remove)
  
  
  #==============================================================================;
  #Stem Words;
  #==============================================================================;
  
  tokens_all_temp <- tokens_all_temp[order(tokens_all_temp[,"wficn"],tokens_all_temp[,"yr"]),] 

  tokens_all_temp_dt <- data.table(tokens_all_temp[(tokens_all_temp[,"Remove"]==0),], key = c("wficn","yr"))
  tokens_all_temp1 <- tokens_all_temp_dt[,list(word=stem_words(token,myStopwords_all)),by="wficn,yr"]
  tokens_all_temp1 <- as.data.frame(tokens_all_temp1,stringsAsFactors=FALSE)
  tokens_all_temp  <- data.frame(wficn=tokens_all_temp1[,"wficn"],
                                 yr=tokens_all_temp1[,"yr"],
                                 token=tokens_all_temp1[,"word"],
                                 Remove=0,stringsAsFactors=FALSE)
  
  rm2(tokens_all_temp_dt,tokens_all_temp1)
  
  #==============================================================================;
  #Find Unique Words for Each WFICN;
  #==============================================================================;
  
  #Sort tokens_all_temp
  tokens_all_temp  <- tokens_all_temp[order(tokens_all_temp[,"wficn"],tokens_all_temp[,"yr"],tokens_all_temp[,"token"]),]
  
  query_tokens_all_temp2 <- ""
  query_tokens_all_temp2 <- paste(query_tokens_all_temp2,"select distinct  wficn,yr, Upper(token) token, Count(token) Count, Remove  ", sep=" ")
  query_tokens_all_temp2 <- paste(query_tokens_all_temp2,"from             tokens_all_temp                                           ", sep=" ")
  query_tokens_all_temp2 <- paste(query_tokens_all_temp2,"where            Remove=0                                                  ", sep=" ")
  query_tokens_all_temp2 <- paste(query_tokens_all_temp2,"group by         wficn, yr, Upper(token)                                   ", sep=" ")
  query_tokens_all_temp2 <- trim(gsub(" {2,}", " ", query_tokens_all_temp2))
  
  #tokens_all_temp2 <- cbind(sqldf(query_tokens_all_temp2), NA, NA, NA)
  #colnames(tokens_all_temp2) <- c("ID","yr","token","Count","Remove","uTotal","gTotal","Total_Percentage")
  
  
  tokens_all_temp2 <- data.frame(sqldf(query_tokens_all_temp2),uTotal=NA,gTotal=NA,Total_Percentage=NA,stringsAsFactors=FALSE)
  
  tokens_all_temp2[,c("uTotal","gTotal","Total_Percentage")] <- subset(data.table(tokens_all_temp2)[, list(uTotal=length(token), gTotal=sum(Count), Total_Percentage=(Count)/sum(Count)),by="yr,wficn"],
                                                                       select=c("uTotal","gTotal","Total_Percentage"))
  
  #==============================================================================;
  cat("SECTION: GLOBAL DICTIONARY (AGGREGATE)", "\n")
  #
  # global_agg_word_grand_temp: total words across ids 
  # global_agg_word_unique_temp: total unique words across ids 
  #
  #==============================================================================;
  
  #identifier <- "ID"
  #identifier <- "crsp_fundno"
  identifier <- "wficn"
  
  #==============================================================================;
  #Create global aggregate grand word table;
  #==============================================================================;
  
  #Copy tokens_all_temp to global_agg_word_grand_temp
  global_agg_word_grand_temp <- tokens_all_temp[tokens_all_temp[,"Remove"]==0,c("yr",identifier,"token")]
  global_agg_word_grand_temp <- global_agg_word_grand_temp[!(rowSums(is.na(global_agg_word_grand_temp[,1:ncol(global_agg_word_grand_temp)]))==ncol(global_agg_word_grand_temp)),]
  global_agg_word_grand_temp[,"yr"] <- 9999
  global_agg_word_grand_temp <- global_agg_word_grand_temp[order(global_agg_word_grand_temp[,"yr"], global_agg_word_grand_temp[,identifier],global_agg_word_grand_temp[,"token"]),] 
  
  global_agg_word_grand_temp3 <- create_global_dictionary_word(global_agg_word_grand_temp,"word_grand",percentiles)
  
  rm2(global_agg_word_grand_temp)
  
  
  #==============================================================================;
  #Create global aggregate unique word table;
  #==============================================================================;
  
  #Copy tokens_all_temp2 to global_agg_word_unique_temp
  global_agg_word_unique_temp <- tokens_all_temp2[tokens_all_temp2[,"Remove"]==0,c("yr",identifier,"token")]
  global_agg_word_unique_temp <- global_agg_word_unique_temp[!(rowSums(is.na(global_agg_word_unique_temp[,1:ncol(global_agg_word_unique_temp)]))==ncol(global_agg_word_unique_temp)),]
  global_agg_word_unique_temp[,"yr"] <- 9999
  global_agg_word_unique_temp <- unique(global_agg_word_unique_temp[,c("yr",identifier,"token")], incomparables=FALSE)
  global_agg_word_unique_temp <- global_agg_word_unique_temp[order(global_agg_word_unique_temp[,"yr"], global_agg_word_unique_temp[,identifier],global_agg_word_unique_temp[,"token"]),] 
  
  global_agg_word_unique_temp3 <- create_global_dictionary_word(global_agg_word_unique_temp,"word_unique",percentiles)
  
  rm2(global_agg_word_unique_temp)

  #==============================================================================;
  #Create global aggregate unique id table;
  #==============================================================================;
  
  #Copy tokens_all_temp2 to global_agg_id_unique_temp
  global_agg_id_unique_temp <- tokens_all_temp2[tokens_all_temp2[,"Remove"]==0,c("yr",identifier,"token")]
  global_agg_id_unique_temp <- global_agg_id_unique_temp[!(rowSums(is.na(global_agg_id_unique_temp[,1:ncol(global_agg_id_unique_temp)]))==ncol(global_agg_id_unique_temp)),]
  global_agg_id_unique_temp[,"yr"] <- 9999
  global_agg_id_unique_temp <- unique(global_agg_id_unique_temp[,c("yr",identifier,"token")], incomparables=FALSE)
  global_agg_id_unique_temp <- global_agg_id_unique_temp[order(global_agg_id_unique_temp[,"yr"], global_agg_id_unique_temp[,identifier],global_agg_id_unique_temp[,"token"]),] 
  
  #Copy tokens_all_temp to unique_ids_agg
  unique_ids_agg <- tokens_all_temp[tokens_all_temp[,"Remove"]==0,c("yr",identifier,"token")]
  unique_ids_agg <- unique_ids_agg[!(rowSums(is.na(unique_ids_agg[,1:ncol(unique_ids_agg)]))==ncol(unique_ids_agg)),]
  unique_ids_agg[,"yr"] <- 9999
  unique_ids_agg <- unique_ids_agg[order(unique_ids_agg[,"yr"], unique_ids_agg[,identifier]),] 
  
  #Get list of all unique words
  unique_ids_agg <- ddply(unique_ids_agg, "yr", function(x) as.data.frame(unique(x[,c("yr",identifier)], incomparables=FALSE)))
  unique_ids_agg <- ddply(unique_ids_agg, "yr", function(x) data.frame(x,nrow=as.numeric(nrow(x))))
  unique_ids_agg <- unique_ids_agg[order(unique_ids_agg[,"yr"], unique_ids_agg[,identifier]),] 
  write.csv(unique_ids_agg, file=paste(output_directory,"unique_ids_agg",readbl_vars[m,2],".csv",sep=""),row.names=FALSE)
  
  global_agg_id_unique_temp3 <- create_global_dictionary_id(global_agg_id_unique_temp,"id_unique",percentiles,unique_ids_agg)
  
  #write.csv(global_agg_id_unique_temp3, file=paste(output_directory,"global_agg_id_unique_temp3",".csv",sep=""))
  
  rm2(global_agg_id_unique_temp)

  
  #==============================================================================;
  #Merge the ID DVs with the Word DVs (aggregate);
  #==============================================================================;
  
  global_agg_comb1 <- merge(global_agg_word_grand_temp3, global_agg_word_unique_temp3, by.x=c("yr","token") , by.y=c("yr","token"), all.x=TRUE, all.y=FALSE, sort=TRUE, suffixes=c(".x",".y"),incomparables=NA)
  global_agg_comb2 <- merge(global_agg_comb1, global_agg_id_unique_temp3, by.x=c("yr","token") , by.y=c("yr","token") , all.x=TRUE, all.y=FALSE, sort=TRUE, suffixes=c(".x",".y"),incomparables=NA)
  
  global_agg_count_vector <- grep(pattern='*Count_*', colnames(global_agg_comb2), ignore.case=FALSE, perl=FALSE, value=TRUE)
  global_agg_dv_vector <- grep(pattern='*_DV_*', colnames(global_agg_comb2), ignore.case=FALSE, perl=FALSE, value=TRUE)
  global_agg_col_vector <- append(global_agg_count_vector,global_agg_dv_vector)
  global_agg_col_vector <- append("token",global_agg_col_vector)
  global_agg_col_vector <- append("yr",global_agg_col_vector)
  global_agg_comb  <- global_agg_comb2[,global_agg_col_vector]
  write.csv(global_agg_comb, file=paste(output_directory,"global_agg_comb",readbl_vars[m,2],".csv",sep=""),row.names=FALSE)
  
  rm2(global_agg_comb1,global_agg_comb2,global_agg_word_grand_temp3,global_agg_word_unique_temp3,global_agg_id_unique_temp3)

  
  #==============================================================================;
  #Create global aggregate tables based on the percentiles;
  #==============================================================================;
  
  global_agg_dv_vector_used <- as.data.frame(cbind(global_agg_dv_vector,rep(NA,length(global_agg_dv_vector))),stringsAsFactors=FALSE)
  colnames(global_agg_dv_vector_used)[2] <- "Column_not_all_0"
  
  #Remove trimmed words and save individual dictionaries
  for (b in 1:length(measures))
  {
    #b <- 1
    
    global_agg_dv_vector_used <- trim_global_dictionary(global_agg_comb,measures[b],percentiles,global_agg_dv_vector_used,readbl_vars[m,2])
    
    progress_function(outer_loop_count=b, outer_loop_start_val=1, outer_loop_end_val=length(measures), inner_loop_count=1, inner_loop_start_val=1, inner_loop_end_val=1)
    
  }
  
  assign(paste("global_agg_dv_vector_used",readbl_vars[m,2],sep=""), global_agg_dv_vector_used, envir=.GlobalEnv)
  
  rm2(global_agg_dv_vector_used)

  
  #==============================================================================;
  cat("SECTION: GLOBAL DICTIONARY (YEARLY)", "\n")
  #==============================================================================;
  
  #==============================================================================;
  #Create global yearly grand word table;
  #==============================================================================;
  
  #Copy tokens_all_temp to global_year_word_grand_temp
  global_year_word_grand_temp <- tokens_all_temp[tokens_all_temp[,"Remove"]==0,c("yr",identifier,"token")]
  global_year_word_grand_temp <- global_year_word_grand_temp[!(rowSums(is.na(global_year_word_grand_temp[,1:ncol(global_year_word_grand_temp)]))==ncol(global_year_word_grand_temp)),]
  global_year_word_grand_temp <- global_year_word_grand_temp[order(global_year_word_grand_temp[,"yr"], global_year_word_grand_temp[,identifier],global_year_word_grand_temp[,"token"]),] 
  
  global_year_word_grand_temp3 <- create_global_dictionary_word(global_year_word_grand_temp,"word_grand",percentiles)
  
  rm2(global_year_word_grand_temp)
  
  
  #==============================================================================;
  #Create global year unique word table;
  #==============================================================================;
  
  #Copy tokens_all_temp2 to global_year_word_unique_temp
  global_year_word_unique_temp <- tokens_all_temp2[tokens_all_temp2[,"Remove"]==0,c("yr",identifier,"token")]
  global_year_word_unique_temp <- global_year_word_unique_temp[!(rowSums(is.na(global_year_word_unique_temp[,1:ncol(global_year_word_unique_temp)]))==ncol(global_year_word_unique_temp)),]
  global_year_word_unique_temp <- global_year_word_unique_temp[order(global_year_word_unique_temp[,"yr"], global_year_word_unique_temp[,identifier],global_year_word_unique_temp[,"token"]),] 
  global_year_word_unique_temp <- unique(global_year_word_unique_temp[,c("yr",identifier,"token")], incomparables=FALSE)
  global_year_word_unique_temp3 <- create_global_dictionary_word(global_year_word_unique_temp,"word_unique",percentiles)
  
  rm2(global_year_word_unique_temp)
 
  
  #==============================================================================;
  #Create global year unique id table;
  #==============================================================================;
  
  #Copy tokens_all_temp2 to global_year_id_unique_temp
  global_year_id_unique_temp <- tokens_all_temp2[tokens_all_temp2[,"Remove"]==0,c("yr",identifier,"token")]
  global_year_id_unique_temp <- global_year_id_unique_temp[!(rowSums(is.na(global_year_id_unique_temp[,1:ncol(global_year_id_unique_temp)]))==ncol(global_year_id_unique_temp)),]
  global_year_id_unique_temp <- global_year_id_unique_temp[order(global_year_id_unique_temp[,"yr"], global_year_id_unique_temp[,identifier],global_year_id_unique_temp[,"token"]),] 
  global_year_id_unique_temp <- unique(global_year_id_unique_temp[,c("yr",identifier,"token")], incomparables=FALSE)
  
  #Copy tokens_all_temp to unique_ids_year
  unique_ids_year <- tokens_all_temp[tokens_all_temp[,"Remove"]==0,c("yr",identifier,"token")]
  unique_ids_year <- unique_ids_year[!(rowSums(is.na(unique_ids_year[,1:ncol(unique_ids_year)]))==ncol(unique_ids_year)),]
  unique_ids_year <- unique_ids_year[order(unique_ids_year[,"yr"], unique_ids_year[,identifier]),] 
  
  #Get list of all unique words
  unique_ids_year <- ddply(unique_ids_year, "yr", function(x) as.data.frame(unique(x[,c("yr",identifier)], incomparables=FALSE)))
  unique_ids_year <- ddply(unique_ids_year, "yr", function(x) data.frame(x,nrow=as.numeric(nrow(x))))
  unique_ids_year <- unique_ids_year[order(unique_ids_year[,"yr"], unique_ids_year[,identifier]),] 
  write.csv(unique_ids_year, file=paste(output_directory,"unique_ids_year",readbl_vars[m,2],".csv",sep=""),row.names=FALSE)
  
  global_year_id_unique_temp3 <- create_global_dictionary_id(global_year_id_unique_temp,"id_unique",percentiles,unique_ids_year)
  
  #write.csv(global_year_id_unique_temp3, file=paste(output_directory,"global_year_id_unique_temp3",".csv",sep=""))
  
  rm2(global_year_id_unique_temp)

  
  #==============================================================================;
  #Merge the ID DVs with the Word DVs (year);
  #==============================================================================;
  
  global_year_comb1 <- merge(global_year_word_grand_temp3, global_year_word_unique_temp3, by.x=c("yr","token") , by.y=c("yr","token"), all.x=TRUE, all.y=FALSE, sort=TRUE, suffixes=c(".x",".y"),incomparables=NA)
  global_year_comb2 <- merge(global_year_comb1, global_year_id_unique_temp3, by.x=c("yr","token") , by.y=c("yr","token") , all.x=TRUE, all.y=FALSE, sort=TRUE, suffixes=c(".x",".y"),incomparables=NA)
  
  global_year_count_vector <- grep(pattern='*Count_*', colnames(global_year_comb2), ignore.case=FALSE, perl=FALSE, value=TRUE)
  global_year_dv_vector <- grep(pattern='*_DV_*', colnames(global_year_comb2), ignore.case=FALSE, perl=FALSE, value=TRUE)
  global_year_col_vector <- append(global_year_count_vector,global_year_dv_vector)
  global_year_col_vector <- append("token",global_year_col_vector)
  global_year_col_vector <- append("yr",global_year_col_vector)
  global_year_comb  <- global_year_comb2[,global_year_col_vector]
  write.csv(global_year_comb, file=paste(output_directory,"global_year_comb",readbl_vars[m,2],".csv",sep=""),row.names=FALSE)
  
  
  #global_year_comb_diff <- global_year_comb[global_year_comb[,11]!=global_year_comb[,14],]
  
  rm2(global_year_comb1,global_year_comb2,global_year_word_grand_temp3,global_year_word_unique_temp3,global_year_id_unique_temp3)
  
  
  #==============================================================================;
  #Create global year tables based on the percentiles;
  #==============================================================================;
  
  global_year_dv_vector_used <- as.data.frame(cbind(global_year_dv_vector,rep(NA,length(global_year_dv_vector))),stringsAsFactors=FALSE)
  colnames(global_year_dv_vector_used)[2] <- "Column_not_all_0"
  
  #Remove trimmed words and save individual dictionaries
  for (b in 1:length(measures))
  {
    #b <- 1
    
    global_year_dv_vector_used <- trim_global_dictionary(global_year_comb,measures[b],percentiles,global_year_dv_vector_used,readbl_vars[m,2])
    
    progress_function(outer_loop_count=b, outer_loop_start_val=1, outer_loop_end_val=length(measures), inner_loop_count=1, inner_loop_start_val=1, inner_loop_end_val=1)
    
  }
  
  assign(paste("global_year_dv_vector_used",readbl_vars[m,2],sep=""), global_year_dv_vector_used, envir=.GlobalEnv)
  
  rm2(global_year_dv_vector_used)
  rm2(tokens_all_temp)
  
  #OUPUT INDIVIDUAL TOKEN DICTIONARIES
  
  global_agg_tokens <- tokens_all_temp2[tokens_all_temp2[,"Remove"]==0,c("yr",identifier,"token")]
  global_agg_tokens <- global_agg_tokens[!(rowSums(is.na(global_agg_tokens[,1:ncol(global_agg_tokens)]))==ncol(global_agg_tokens)),]
  global_agg_tokens[,"yr"] <- 9999
  global_agg_tokens <- unique(global_agg_tokens[,c("yr",identifier,"token")], incomparables=FALSE)
  global_agg_tokens <- global_agg_tokens[order(global_agg_tokens[,"yr"], global_agg_tokens[,identifier],global_agg_tokens[,"token"]),] 
  write.csv(global_agg_tokens, file=paste(output_directory,"global_agg_tokens",readbl_vars[m,2],".csv",sep=""),row.names=FALSE)
  
  global_year_tokens <- tokens_all_temp2[tokens_all_temp2[,"Remove"]==0,c("yr",identifier,"token")]
  global_year_tokens <- global_year_tokens[!(rowSums(is.na(global_year_tokens[,1:ncol(global_year_tokens)]))==ncol(global_year_tokens)),]
  global_year_tokens <- unique(global_year_tokens[,c("yr",identifier,"token")], incomparables=FALSE)
  global_year_tokens <- global_year_tokens[order(global_year_tokens[,"yr"], global_year_tokens[,identifier],global_year_tokens[,"token"]),] 
  write.csv(global_year_tokens, file=paste(output_directory,"global_year_tokens",readbl_vars[m,2],".csv",sep=""),row.names=FALSE)
  
  rm2(tokens_all_temp2)

  
  #==============================================================================;
  #CREATE PROGRESS OUTPUTS;
  #==============================================================================;
  
  rm2(global_agg_comb,unique_ids_agg,global_agg_tokens)
  rm2(global_year_comb,unique_ids_year,global_year_tokens)

  progress_function(outer_loop_count=m, outer_loop_start_val=1, outer_loop_end_val=nrow(readbl_vars), inner_loop_count=1, inner_loop_start_val=1, inner_loop_end_val=1)
  
  #END OF M FOR LOOP
  
}

rm2(sample_data_all)
rm2(remove_descriptions,remove_punct,remove_phone,remove_tokens)
rm2(keep_one_letter_words, keep_one_letter_ratings, keep_one_letter_tokens)
rm2(keep_two_letter_words, keep_state_abbreviations, keep_two_letter_ratings, keep_two_letter_tokens)

#==============================================================================;
cat("SECTION: INDIVIDUAL DICTIONARY", "\n")
#==============================================================================;

identifier <- "wficn"

output_db <- paste(output_directory,"Text_Analysis.s3db",sep="")

for (m in 1:nrow(readbl_vars))
{
  
  #m <- 1
  #m <- 2
  
  if (m==1)
  {
    file_type_str <- readbl_vars[m,2]
    
  } else if (m==2)
  {
    file_type_str <- readbl_vars[m,2]
    
  } else
  {
    cat("ERROR WHEN FINDING CORRECT READABILITY COLUMN", "\n")
    
  }
  
  #AGGREGATE
  combined_data <- as.data.frame(fread(paste(output_directory,"global_agg_comb",file_type_str,".csv",sep=""),na.strings="NA",stringsAsFactors=FALSE),stringsAsFactors=FALSE)
  tokens_data <- as.data.frame(fread(paste(output_directory,"global_agg_tokens",file_type_str,".csv",sep=""),na.strings="NA",stringsAsFactors=FALSE),stringsAsFactors=FALSE)
  id_data <- as.data.frame(fread(paste(output_directory,"unique_ids_agg",file_type_str,".csv",sep=""),na.strings="NA",stringsAsFactors=FALSE),stringsAsFactors=FALSE)
  
  for (a in 1:length(measures))
  {
    
    for (b in 1:nrow(percentiles))
    {
      #a <- 1
      #b <- 1
      
      create_individual_dictionary(combined_data,"agg",file_type_str,tokens_data,id_data,measures[a],percentiles[b,],output_db,output_directory,identifier)
      
      progress_function(outer_loop_count=a, outer_loop_start_val=1, outer_loop_end_val=length(measures), inner_loop_count=b, inner_loop_start_val=1, inner_loop_end_val=nrow(percentiles))
      
    }
  }
  
  rm2(combined_data,tokens_data,id_data)
 
  #YEARLY
  combined_data <- as.data.frame(fread(paste(output_directory,"global_year_comb",file_type_str,".csv",sep=""),na.strings="NA",stringsAsFactors=FALSE),stringsAsFactors=FALSE)
  tokens_data <- as.data.frame(fread(paste(output_directory,"global_year_tokens",file_type_str,".csv",sep=""),na.strings="NA",stringsAsFactors=FALSE),stringsAsFactors=FALSE)
  id_data <- as.data.frame(fread(paste(output_directory,"unique_ids_year",file_type_str,".csv",sep=""),na.strings="NA",stringsAsFactors=FALSE),stringsAsFactors=FALSE)
  
  for (a in 1:length(measures))
  {
    
    for (b in 1:nrow(percentiles))
    {
      
      #a <- 3
      #b <- 1
      
      create_individual_dictionary(combined_data,"year",file_type_str,tokens_data,id_data,measures[a],percentiles[b,],output_db,output_directory,identifier)
      
      progress_function(outer_loop_count=a, outer_loop_start_val=1, outer_loop_end_val=length(measures), inner_loop_count=b, inner_loop_start_val=1, inner_loop_end_val=nrow(percentiles))
      
    }
  }
  
  rm2(combined_data,tokens_data,id_data)
  
}

rm2(output_db)

#SQLite Test

#Text_Analysis_tables <- ListTables("Text_Analysis.s3db")

#Text_Analysis_fields <- ListFields("Text_Analysis.s3db")
#write.csv(Text_Analysis_fields, file=paste(output_directory,"Text_Analysis_fields",".csv",sep=""), row.names=FALSE)

#sql_q <- paste("SELECT * FROM ",ta_db_tables[1],"",sep="")
#aaa <- runsql(sql_q,"Text_Analysis.s3db")
#rm2(aaa)


#agg_id_unique_sim_cosine_normalized_990pct_iois_9999 <- runsql("SELECT * FROM agg_id_unique_sim_cosine_normalized_990pct_iois_9999","Text_Analysis.s3db")
#ExportTable(agg_id_unique_sim_cosine_normalized_990pct_iois_9999,paste(output_directory,"Similarity_Analysis.s3db",sep=""))
#runsql("DROP TABLE agg_id_unique_sim_cosine_normalized_990pct_iois_9999","Text_Analysis.s3db")
#rm2(agg_id_unique_sim_cosine_normalized_990pct_iois_9999)


#Similarity_Analysis_tables <- ListTables("Similarity_Analysis.s3db")

#==============================================================================;
cat("COMPUTE SIMILARITY MEASURES", "\n")
#==============================================================================;

identifier <- "wficn"

sample_data_all <- read.csv(file=paste(output_directory,"sample_data_all.csv",sep=""),header=TRUE,na.strings="NA",stringsAsFactors=FALSE)
for(i in which(sapply(sample_data_all,class)=="character"))
{
  sample_data_all[[i]] = trim(sample_data_all[[i]])
}
for (i in 1:ncol(sample_data_all))
{
  sample_data_all[,i] <- unknownToNA(sample_data_all[,i], unknown=c("",".","NA_character_","NA_Real_","NA",NA),force=TRUE)
  sample_data_all[,i] <- ifelse(is.na(sample_data_all[,i]),NA, sample_data_all[,i])
} 

input_db <- paste(output_directory,"Text_Analysis.s3db",sep="")
output_db <- paste(output_directory,"Similarity_Analysis.s3db",sep="")

#input_db_tables <- ListTables(input_db)
#input_db_fields <- ListFields(input_db)

#temp_fields <- input_db_fields[input_db_fields[,1]=="year_word_grand_expand_norm_990pct_iois",]

for (m in 1:nrow(readbl_vars))
{
  
  #m <- 1
  #m <- 2
  
  if (m==1)
  {
    file_type_str <- readbl_vars[m,2]
    
  } else if (m==2)
  {
    file_type_str <- readbl_vars[m,2]
    
  } else
  {
    cat("ERROR WHEN FINDING CORRECT READABILITY COLUMN", "\n")
    
  }
  
  #AGGREGATE
  for (a in 1:length(measures))
  {
    
    for (b in 1:nrow(percentiles))
    {
      calculate_cosine_similarity("agg",file_type_str,measures[a],percentiles[b,],input_db,output_db,output_directory,"cosine_normalized",sample_data_all,identifier)
      
      progress_function(outer_loop_count=a, outer_loop_start_val=1, outer_loop_end_val=length(measures), inner_loop_count=b, inner_loop_start_val=1, inner_loop_end_val=nrow(percentiles))
      
    }
  }
  
  capture.output(gc(),file='NUL')
  
  #YEARLY
  for (a in 1:length(measures))
  {
    
    for (b in 1:nrow(percentiles))
    {
      #a <- 1
      #b <- 3
      
      calculate_cosine_similarity("year",file_type_str,measures[a],percentiles[b,],input_db,output_db,output_directory,"cosine_normalized",sample_data_all,identifier)
      
      progress_function(outer_loop_count=a, outer_loop_start_val=1, outer_loop_end_val=length(measures), inner_loop_count=b, inner_loop_start_val=1, inner_loop_end_val=nrow(percentiles))
      
    }
  }
  
  capture.output(gc(),file='NUL')
  
}

rm2(input_db,output_db)

#Similarity_Analysis_tables <- ListTables("Similarity_Analysis.s3db")

#Similarity_Analysis_fields <- ListFields("Similarity_Analysis.s3db")
#write.csv(Similarity_Analysis_fields, file=paste(output_directory,"Similarity_Analysis_fields",".csv",sep=""), row.names=FALSE)

#sql_q <- paste("SELECT * FROM ",ta_db_tables[1],"",sep="")
#aaa <- runsql(sql_q,"Text_Analysis.s3db")
#rm2(aaa)

#agg_id_unique_sim_cosine_normalized_990pct_iois_9999 <- runsql("SELECT * FROM agg_id_unique_sim_cosine_normalized_990pct_iois_9999","Text_Analysis.s3db")
#ExportTable(agg_id_unique_sim_cosine_normalized_990pct_iois_9999,paste(output_directory,"Similarity_Analysis.s3db",sep=""))
#runsql("DROP TABLE agg_id_unique_sim_cosine_normalized_990pct_iois_9999","Text_Analysis.s3db")
#rm2(agg_id_unique_sim_cosine_normalized_990pct_iois_9999)


#==============================================================================;
#DONE;
cat("DONE", "\n")
#==============================================================================;
